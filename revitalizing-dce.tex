% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}
\usepackage{array}
\usepackage{tikz}
\usepackage{color}
\usepackage{hyperref}
%\usepackage{xcolor}
\usepackage{listings}
\newcommand{\subparagraph}{}
%\usepackage{titlesec}

\lstdefinestyle{CStyle}{
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}

\setcounter{secnumdepth}{4}

%\titleformat{\paragraph}
%{\normalfont\normalsize}{\theparagraph}{1em}{}

\begin{document}
%
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Revitalizing ns-3's Direct Code Execution}

%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Parth Pratim Chatterjee\\
       \affaddr{Kalinga Institute of Industrial Technology}\\
       \email{parth27official@gmail.com}
\alignauthor
Thomas R. Henderson\\
       \affaddr{University of Washington}\\
       \email{tomhend@u.washington.edu}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.

% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
%  * What was done? 
This paper describes the design, implementation and validation
of the ns-3 model of the Licklider Transmission Protocol, the standard
transport protocol used to provide transmission reliability in Delay Tolerant Networks (DTNs).
%  * Why do it? 
DTNs are an emerging field whose principles are used to provide communications 
in extreme and performance-challenged environments, such as spacrecraft,underwater, or
disaster response scenarios. Evaluation of such environments requires the use of simulation tools.
As of now, there is a lack of precise simulation models of these protocols, and concretely within the ns-3 simulator.
%  * What were the results?
The ns-3 model presented in this paper accurately models the LTP protocol and offers ...
\end{abstract}

% A category with the (minimum) three required fields
%\category{C.2.2}{ Computer-Communication Networks }{Network Protocols} [Protocol architecture]
%A category including the fourth, optional field follows...
%\category{I.6.5}{ Simulation and Modeling }{ Model Development}

%\terms{Theory}

\keywords{ns-3}

\section{Introduction}

\begin{itemize}
\item Introduce ns-3
\item Motivation of DCE: make complex network implementations available to ns-3
\item History of DCE, and challenge of maintaining it
\end{itemize}
% https://www.theregister.com/2020/01/06/linux_2020_kernel_systemd_code/
% estimated 27.8 million lines of code with 75,000-80,000 commits per year.

The rest of this paper is organized as follows: Section 2 provides an overview on the technical challenges due to Linux evolution.
Section 3 describes solutions.
Section 4 presents the results.  Section 5 related work.  Section 6 conclusions
and future work.

\section{Challenges}

\subsection{libio vtable mangling}
The vtable is a table maintaining references to functions called for virtual functions defined for a class or an entity. These functions can 
be overriden dynamically by user defined functions and the respective call for the corresponding virtual function in a derived class object can be bound 
to that function at runtime, unlike pre-defined functions which are static, fixed, and can be determined during compile time. The libc on Linux provides
this highly flexible feature for most user defined classes, but the case with the FILE structure is not the same.

The FILE structure is a library defined structure which defines the overall organization, orientation and properties of any file I/O stream opened
by the host application. It maintains different parameters for storing useful operational fields like the UNIX based file descriptor number of the 
opened stream, the read/write offsets and buffer addresses of the stream. The pseudoname for the FILE structure as seen inside libc is \textit{\_IO\_FILE}. 
Since, FILE is a library defined entity, the library provides its own set of implementation for all possible operations on an open FILE stream.
Whenever an \textit{\_IO\_FILE} stream is allocated by the kernel, a contiguous memory location is allocated as a block called \textit{\_IO\_FILE\_plus}. 
The \textit{\_IO\_FILE\_plus} structure looks like this.

\begin{lstlisting}[style=CStyle]     
struct _IO_FILE_plus
{
  FILE file;
  const struct _IO_jump_t *vtable;
};
\end{lstlisting}

By nature of the implementation of the kernel's memory allocation processes, the contents of a struct are allocated in contiguous memoory locations. 
This can be verified by the \textit{sizeof} operation of C showing that the sum of sizes of the individual members of a struct is equal to the size of
the struct object. Similarly, the FILE and the \textit{\_IO\_jump\_t} objects are allocated in contiguous memory locations. Specifically, the 
\textit{\_IO\_jump\_t} areas is interesting to DCE, as it defines the callbacks or reference pointers to the functions handlers for each supported file 
operation. Some of the callbacks which are interesting to DCE and its use cases are highlighed below.

\begin{lstlisting}[style=CStyle]     
struct _IO_jump_t
{
...
ssize_t(*) __read (FILE *, void *, ssize_t);
ssize_t(*) __write (FILE *, const void *, ssize_t);
off64_t (*) __seek (FILE *, off64_t, int);
int (*) __close (FILE *);
int (*) __stat (FILE *, void *);
...
};
\end{lstlisting}

This structure acts like the vtable for the FILE structure, but it does not behave like the oridinary vtable seen when working with virtual functions 
and derived classes, which are dynamic and supports run time bindings. This vtable is instead expected to behave as a statically bound vtable.  There exist 
other libc functions, like \textit{fopencookie}, to override some of the FILE operation implementations, but not all operations are supported, and 
\textit{fopencookie} also does not attach itself to a standard file I/O stream, 
instead working with user defined buffers a.k.a cookies). 

A key aspect of DCE is that it needs to \textit{hijack}
application operations like system calls, file I/O operations, and networking
system calls, and re-route them through 
corresponding handlers based on the application logic and simulation script implementation. Considering file I/O operations, DCE needs to have control 
over read/write/close/seek/stat operations of each open file, which requires
overwriting the vtable handlers with the corresponding handlers defined 
in DCE's stdio definition source files. 

Taking advantage of the contiguous memory allocation of the FILE and \textit{\_IO\_jump\_t}, one can overwrite the vtable with a custom vtable definition for all the operations needed for DCE. One can make a dummy \textit{\_IO\_FILE\_plus} 
pointer point to the explicitly casted FILE object. \textit{memcpy} the existing vtable to a local copy, modify and overwrite the stream operations with a 
custom written implementation, and then re-point the vtable field of the dummy \textit{\_IO\_FILE\_plus} to the local modified vtable.  This allows
control over stream operations, which can now be routed through and and to behave as ns-3 streams, Unix FD streams, etc. based on the type of file 
descriptor that is defined.  Although this control over FILE streams
to regulate stream buffer flushing and data redirection is productive for
DCE, these techniques can also be used maliciously for
\textit{buffer overflow attacks}, as it lets penetration 
testers to make use of tools like pwntools etc. to gain control over application execution and important run time CPU register values such as the 
\textit{rip} which is used for the relative addressing of application components(which is also how position-independent-executables work), which is more 
secure as compared to static addressing, where fixed address values of symbols and pointers could be gained by static analysis tools for run 
time application exploitations.

Post libc-2.25, security features have been implemented to glibc to identify 
and block buffer overflow attacks on the FILE object. Whenever any FILE
operation is executed, glibc 
verifies if the FILE object's vtable can be trusted and is not corruputed or manipulated. To verify this, it makes a call to 
\textit{\_IO\_validate\_vtable}. Every libio vtable is defined in a unique section called \textit{libio\_IO\_vtables}. By definition, libc trusts
the vtable if the vtable of the current FILE object lies within this section. It checks if the offsets of this vtable lies between 
\textit{\_\_stop\_\_\_libc\_IO\_vtables} and \textit{\_\_start\_\_\_libc\_IO\_vtables}, if it does, we can continue with the operation, if not, libc 
conducts a final check by calling \textit{\_IO\_vtable\_check} which makes final checks on the FILE vtable pointer location, namespace and edge cases
where FILE * objects are passed to threads which are not in the currently linked executable.  When DCE overflows the \textit{\_IO\_FILE\_plus} and 
overwrites the \textit{\_IO\_jump\_t}, it does not lie in \textit{libio\_IO\_vtables} section and it also does not pass the pointer mangling sanity checks, leading to a \textit{\_\_libc\_fatal (\"Fatal error: glibc detected an invalid stdio handle\");}

\subsection{PIE loading and usage}
Position-independent-executables (PIE) are applications compiled with special compiler flags that allow the application to be loaded into an arbitrary
memory address, not depending on absolute symbol addresses.  PIE executables
avoid exploits that hijack the call stack by referencing fixed function/symbol addresses that can be found in executables not compiled in this manner.
Every memory address is accessed with reference to what is called the \textit{\%rip} (instruction pointer). The \textit{\%rip} is computed at the time 
of execution when the application is loaded into virtual memory. This approach makes it difficult for attackers to determine symbol locations in memory.

DCE supports the execution of real host applications in simulation, bridging the networking layer between the host and the specified networking stack,
and also supporting other system calls made by the host applications. Since several applications may need to be loaded into memory, and also have control over the
position of the \text{main} symbol of the loaded application, DCE needs to have position independent executables, so that when they are loaded into memory,
the symbol positions in memory are dynamic, giving control over when an application is launched in a simulation (which can be configured in the script 
using available ns-3 programming constructs for the \textit{DceApplicationManager} class). To implement this, DCE uses the \textit{CoojaLoader} which 
uses \textit{dlmopen} under the hood to load the executable into memory. In glibc library versions newer than 2.25, security checks have been introduced to identify such 
PIE objects being loaded through \textit{dlmopen}, and in case the \textbf{DF\_1\_PIE} flag is found in the object's ELF Dynamic headers, it will abort with an error.
 

\subsection{Linux Networking Stack for DCE}
DCE also provides implementation options for ns-3 simulations for the networking stack at the TCP/IP layers.  Linux and FreeBSD stacks are available as alternatives to the native ns-3 TCP/IP implementation.
Script writers can configure the \textit{DceManager} class to use the chosen stack.
Currenly, the Linux network stack is based on a project named \textit{net-next-nuse-4.4.0}, which is built on top of base Linux kernel version 4.4.0.

The process for supporting portions of the Linux or FreeBSD kernel in DCE is
much more extensive than that for a typical user-space application, because of
the size of the kernel and because the kernel provides its own scheduler and has internal implementations of functions such as memory management.  It is not
simply a matter of compiling kernel code as PIE library code; more intrusive
changes are necessary. 
\textit{net-next-nuse} is a library 
port of the Linux kernel that performs selective linking of required kernel modules and components such as the networking, virtual file system (VFS), memory management unit (MMU), and other layers.  The library port also performs additional functions such as
exporting DCE useful callback structures, abstracting the internal network data flow, coordinating Linux kernel synchronization, 
process creating, and DCE-kernel task, IRQ and tasklet scheduling and synchronization, to expose a Linux-like execution environment for host applications. 

Since Linux kernel 4.4.0, Linux kernel releases have had major developements in almost all kernel components, but for our specific use case,
the networking stack has seen major changes in several components, including the TCP timer, jiffies and clock HZ usage, packet processing (napi) framework, internal enums and state definitions and checks, packet flows, checksum and
offload hardenings, newer congestion control algorithms, ucounts API, etc. 
The Linux kernel version 4.4 networking stack is useful for some ns-3 simulation work but lacks implementations found in more recent kernels that are of interest to current researchers.  Therefore, to upgrade the supported kernel version to something more current, we considered both an evolution of the existing approach, as well as an evaluation of two similar projects, LKL and LibOS, described next. 

\subsubsection{LKL}
The Linux Kernel Library (LKL) is a library port of the Linux kernel that, through some pre-shipped helper shell scripts, can be used to hijack
all system calls made by a host application and map them through the ported Linux kernel rather than through system defined implementations. It also allows one 
to setup network interfaces such as TAP, RAW, VDE , etc. with custom gateways, netmasks, IP addresses, etc. with the help of JSON configuration files. These helper
shell scripts make use of \texttt{LD\_PRELOAD} to reorder library loading to LKL written system calls to take control in place of the libc defined routines.
 
\subsubsection{LibOS}
LibOS uses the same internal architecture as does \textit{net-next-nuse}.
It is also a Linux kernel port that works on the principle of selective 
kernel module linking and patching. It defines special link time constructs
to include only specific kernel files and 
symbols that are needed for executing on top of the Linux kernel with \textit{nuse}, which works similar to LKL by hijacking system calls 
and rerouting them through nuse and kernel defined routines. Only kernel components that let LibOS start the kernel and run all \textit{\_\_initcall}(s),
which are defined with a \textit{\_\_init} and registered as as a initcall using special macros, are linked. These routines are linked into special \textit{.INIT} 
setions of the final linked library. These components include critical parts such \textit{kernel}, \textit{net} and other selective parts of \textit{proc},
\textit{mm}, \textit{fs} , \textit{drivers}, etc. 

\subsubsection{LKL vs. LibOS}
We conducted a comparison of LKL and LibOS approaches according to different design parameters and considerations for a complex application framework such as
DCE with very specific demands from it's underying network stack.

\paragraph{Linux Kernel Support}
LKL, which was primarily designed to work as a Linux-as-userspace-library interface for applications to be dynamically linked to at runtime, is built on top of 
Linux 5.2. The kernel port design of LKL facilitates kernel version upgrades with little to no effort. Abstractly, the kernel upgrade process 
would permit a git rebase on the kernel version the user would want to use, and the project should compile with no major issues to deal with (some minor compiler, 
and Linux kernel header definition changes might come up, which should be resolvable with a bit of effort).

LibOS, which makes use of dynamic, selective Linux kernel linking, bridges the gap between application workspace and Linux kernel networking stack with
the help of glue code, kernel component connector code, and user application provided exported functions and callbacks for proper execution. This
architecture required LibOS to modify some of the internal Linux kernel files for additional components such as the slab memory allocator, which requires 
LibOS to setup preprocessor directives to select a slab allocator for specific Kconfig-defined compiler directives, to pass on control to LibOS routines 
whenever required. Currently, LibOS supports Linux kernel version 4.4.0. Upgrading to a newer Linux kernel version might be an intense process requiring one 
to deal with issues from header file changes, to complete changes in kernel components like the networking layer, memory maqnagement, namespace manager 
and kernel boot process.


\paragraph{In-Library Kernel Boot Order}
Apart from certain functions that are initialized only in a user OS, such as hardware drivers and devices, network buses, NICs, etc., LKL spins up a high level CPU lock controlled thread, which makes calls to 
\texttt{start\_kernel}. Since LKL is a uniprocessor system, it initializes the kernel on a singular LKL thread, locks of 
which are synchronized with Linux scheduler calls, which are called when the scheduler decides to switch execution control 
to kernel level tasks for preemption, and other tasks, which require certain memory level moderation to achieve
atomic operation. Also, since the LKL CPU thread needs to be initialized before any Linux functions can be used by the 
application that is using the LKL library, it eventually disrupts the flow of DCE scripts, which work on a 
scheduling algorithm and specific ns-3 task context switch paradigms that are different from the Linux kernel,
making it wait for LKL's internal Linux kernel opertions to finish, before it can schedule other ns-3 lightweight
threads.  The net result of this tension is to cause simulation result to differ from real world observations.

LibOS does not depend on the actual \texttt{start\_kernel}, particularly because, as opposed to LKL which sets up an environment 
for the Linux kernel, all of the basic requirements for the kernel to assume an actual Linux workspace making it easy 
for LKL to access most of Linux functionalities without any change to kernel internals, LibOS on the other hand makes 
use of its own \texttt{lib\_init} fuction which calls specific setups calls required by the network stack of the kernel to 
work, such as proc, VFS, ramfs, scheduler, etc. It also overrides scheduler member functions, syncing it with ns-3's 
scheduler. LibOS creates an ns-3 task copy for each kernel task which is created by the \texttt{copy\_process}, \texttt{create\_process},
etc. kernel functions. Each such task maps with itself a callback function which should be called once the wait time
for a task is over, or has been invoked as a part of a regular scheduling process. Once tasks are processed, they are 
also popped off the ns-3 task queue.
\paragraph{Maintenance}
LKL was designed to be a low-maintenance, Linux-as-a-userspace-library interface, which exposes Linux subsystems through overidden system calls. 
Since LKL is an architecture-level port of the Linux kernel, isolating LKL specific code to the arch and tools Linux folders, moving on to 
newer or custom releases induces less cost as compared to LibOS, which works on dynamic component compiling and linking the build system through a
custom Makefile, specifying Linux files and modules to be compiled and linked to the final nuse shared object file, and the selection of files to compile must be complete, assuring that no function calls are being made by any critical component to another component that does not 
exist, and as necessary, writing glue code to fulfill the depedency graph for all such modules and files. This makes migrating to newer versions of Linux kernel more time consuming, and 
even simply debugging kernel internals requires more effort since one is not sure if it is the glue code,
the allocator, or the ns-3 synced scheduler that is malfunctioning.

\section{Solutions}
\label{section:design}

\subsection{Custom glibc-2.31-based build}
Given that the standard glibc library, since version 2.25, contains security features that block how DCE makes use of it, the alternatives are to either use a different standard library implementation without such features, or to use a modified version of glibc.  Two options are \textit{libc} and \textit{musl}.
The post-linking structure
of the generated binary and the libary vary, in terms of the number of static linkages, symbol tables, etc. For instance, musl works on the idea of
single static linkage, in which, the library or executable compiled with musl-gcc, is statically linked to only one musl linked library called 
\texttt{ld-musl.so.1}, which defines all the symbols required by the application. This reduces the size of the executable by a huge extent, but 
also does more harm than good to DCE. The initial build step of DCE includes calling a script named \texttt{dcemakeversion.c}. This script is responsible 
for extracting the symbol table of the libc currently being linked to (all symbols for the libraries libc, libpthread, librt, lib and libdl). These 
libraries are the various modular extensions of the glibc providing features such as pthreads, math, dynamic library loading and the base libc 
library as well. The symbols are read from the Elf headers of the respective shared library .so file, and stored in a local .version file.
 
The symbol table for all of these libraries are important, as DCE generates a shared library called \texttt{libc-ns3.so} which is a 
for the local libc and DCE implementations, on top of which host applications are executed. This shared library defines 
all symbols which are defined in the local libc, as \texttt{NATIVE} and all the features implemented inside DCE as \texttt{DCE}. All other symbols that are new to DCE but not implemented by DCE, but that are a part of the local system libc, are then referenced in the .version files.  These symbols are then defined 
as well, to avoid any runtime symbol lookup errors. DCE then generates preprocessor mappings for all of the symbols. All DCE defined symbols will 
natively be mapped to DCE implemented versions of them, rather than to the system libc implementation, and all NATIVE defined symbols will be mapped 
to global namespace implementations, which are the ones already implemented in the system libc. Users would compile their applications on top of 
the system libc itself, but with an extra -fPIC and -pie flag, which allows us to load the applications dynamically into the DCE process address 
space. The next step is to load out the libc-ns3.so shared object file, and to call the \texttt{libc\_setup} function, which initializes the system call mappings. DCE also 
subsequently loads other libraries that have been generated in a similar way, such as \texttt{libpthread-ns3.so}, \texttt{librt-ns3.so}, \texttt{libm-ns3.so}, and \texttt{libdl-ns3.so}.
As a final step, the host application is loaded and the main function of the application is called, which starts to now work on top of the custom libraries.
\texttt{musl-gcc}, on the other hand, does not support modular libraries for these features, and for all cases would link the single \texttt{ld-musl.so.1}, making it 
impossible to segregate the symbols for the different libraries. glibc is a better match for what DCE requires, and links all the standard libraries needed.

To override the vtable mangling security checks of glibc, it is necessary to use a modified libc version that matches the system's default libc version,
so that symbol lookup errors are avoided at runtime by applications built on a different library and being loaded into an available namespace using \texttt{dlmopen}.
It is also necessary to override the security checks on vtable pointer mangling. The gcc compiler and linker options could be used to 
reconfigure the default build environment to build DCE on top of a customized
glibcc. The default root directory where gcc starts to look for
libraries, header files, etc. is  \texttt{'/'} on Linux.  It is necessary to repoint this directory to the custom glibc root. This is where the \texttt{--sysroot} option 
is used to set it to correct bake build directory. Following this, one can then add the custom glibc prioritized directory for library and header file lookup using the 
-L and -I options, respectively. Next, the rpath and rpath-link paths for ELF executables that could be linked to the shared objects at run time 
or link time, respectively, are set. Finally,the dynamic linker is set to the newly built library, using the -Wl--dynamic-linker flag.  All these changes
are placed under an unclosed -Wl --start-group, as DCE requires other linker flags, which can be added before inserting the ending -Wl --end-group.
 


\subsection{Bake Build Automation}

Bake is a Python-based build system invented for DCE.  It orchestrates the build and local installation of several DCE dependencies before managing the DCE build itself, by calling other build tools native to the respective projects, such as make, Waf, or CMake.  Bake works on a build configuration script called \texttt{bakeconf.xml}. The bakeconf.xml file is converted to \texttt{bakefile.xml} after the configuration file is 
parsed by the Bake module. This creates a build dependency graph, maintaining build steps, parameters and post build 
commmands for each dependency node. Bake dynamimcallly binds dependency modules based on configuration options specified for each build module 
defined in the bakeconf.xml. The three major steps of a Bake build are:
1) configure, 2) download and 3) build. Bake supports almost all major types of 
source code fetching methods, including git, mercurial and the fetching of compressed archive files. In the build step, dependencies 
are built in the required order, starting with modules having no dependencies on other modules, and finishing with the module having all dependencies already satisfied. To support 
the custom glibc build, the dependency graph looked something like this: 

ToDo : Attach Bake Dependency graph ? 

Bake has a source configuration option named \textit{patch}, which can safely apply a patch, without re-applying if it has already been applied before.  This allows, for example, a large codebase to be fetched in its unaltered form, while Bake must maintain and apply only a small patch file to it.
Using this option, Bake applies the DCE-specific glibc patch, which disables the security checks on vtable mangling, and also disables the 
PIE object checks for dlmopen position independent executable loading. The glibc is then build using its standard build steps. The linux kernel headers
files are then installed into the \texttt{/usr} directory of a custom glibc's sysroot. This finishes with a standard Linux-like system root to use for building 
DCE without any issues. 


\subsection{Docker environnment for DCE}

\subsection{net-next-nuse-5.10}
Net-next-nuse, introduced above, is an architecture port of the Linux kernel, which resorts to selective kernel module linking and which takes control over critical kernel 
components such as the slab allocator, task scheduler, workqueue-waitqueue handling, timer based function invocation, as well as some kernel 
utilities such as jiffies and random number generators. It also sets up emulations of the network system calls for both general socket networking 
operations as well netdevice based operations. All of this is exposed through an API initialised by a simulator initialisation call, \texttt{sim\_init}, 
which does a bidirectional mapping of the imported (DCE to Linux) and exported (Linux to DCE) function utilities.

\subsubsection{Background}
The implementation of net-next-nuse considers only the use case of DCE and thus might not find application outside of DCE, unlike LKL which
is designed for running a plethora of networking applications, but it fits in really well when it comes to DCE. 

When Linux builds, apart from all other files, it generates two important files: \texttt{vmlinuz} and \texttt{initramfs}. The file vmlinuz is where all the kernel 
code is compiled and linked. LKL performs a raw objcopy on the generated object 
files vmlinux and copies all symbols from vmlinux along with the global functions exported from the LKL arch port, and combines them into a final 
library called \texttt{lkl.o}, which is then passed on to the tools/lkl, to link the host, netdevice, utility, filesystem, and networking handler codes to 
make the final \texttt{liblkl.so} shared library. In the initial stages of the DCE modernization effort, this approach sounded promising, but as explained below, it can become a bottleneck.

In contrast, net-next-nuse understands that the way Linux Kernel behaves on the host, and the way the kernel interacts with the task 
scheduler and how the scheduler operates on the tasks, is completely different from how DCE manages fibers (called tasks in DCE terminology). 
DCE is based on context-based, lightweight threads called fibers, which require a custom scheduler, called the TaskManager in DCE. On an ordinary
host machine, when a user runs any application that spins up normal threads, the user does not need to bother about scheduling the threads on 
the different cores of the host CPU and maintaining mutex locks of the CPU, making the kernel responsible for jumping from one thread to another. 
But when using fibers, one can jump from one thread to another only when one of the fibers yields to another fiber, and this context switch 
is done by a custom scheduler, which is written in DCE's TaskManager::Schedule(). This is beneficial because when using ordinary threads, 
the number of context switches the CPU has to do (push your thread data stack on to you address space, in and out), 
is a lot more expensive than making switches in fibers.

Another challenge is as follows.  Separate schedulers exist inside DCE and within the Linux kernel library. 
As a result, every new operation that you make inside the kernel creates something called a (virtual) kernel thread. 
However, managing those threads is challenging because the DCE TaskManager does not have visibility to the (virtual) threads.
The net-next-nuse approach to solving this issue is to perform  a selective linking of 
kernel modules and utilities and to avoid linking the scheduler, workqueue, waitqueue and other such linked components, opting to instead implement
them using DCE imported functions mapped to TaskManager utilities. As a result, every time that some process wants to preempt and block, the kernel calls 
\texttt{schedule()} or \texttt{schedule\_timeout()}, and control is passed to DCE to take care of it. 

\subsubsection{Slab Allocator}
When the kernel is booted up by net-next-nuse in \texttt{lib\_init(...)}, it calls some of the required initialization functions, and all included, initcalls. 
Most of these initcalls require the creation of a memory cache object called \texttt{kmem\_cache}. These memory slabs are used to allocate memory to child 
objects, using functions like \texttt{kmem\_cache\_alloc(...)} and functions as simple as \texttt{kmalloc(...)} to allocate space for a pointer object. The 
kernel already has slab allocators (two such allocators are SLUB and SLOB), but all of them allocate space internally, giving no control to DCE. We thus use a custom 
SLAB allocator called SLIB, which calls internal \texttt{DCE's malloc(...)} functions and sets up \texttt{kmem\_cache} data structures and constructors etc. It also 
implements compound and single page operations like\texttt{\_put\_page(...)}, \texttt{kfree(...)} and array space allocation. 

\subsubsection{Virtual Kernel Task and its Real Fiber Equivalent}
Thread management in the kernel is another important consideration.
Every new task inside the kernel-- for example, a syscall-- 
creates an internal kernel thread.  When a system boots up, there has to be some initial task that invokes the \texttt{start\_kernel(...)} 
function. This is called the \texttt{init\_task}. Whenever a new kernel thread is to be created, a call to \texttt{kernel\_thread(...)} is made. Along with the 
function that is to be invoked in the thread, and the arguments to be passed, one must also pass along one more argument, known as clone flags. 
Internally, the kernel will try to clone a previous task as much as possible. These flags basically determine 
how far the kernel should go, as in cloning the structures. Some of the flags are \texttt{CLONE\_FS}, \texttt{CLONE\_VM}, \texttt{CLONE\_FILES}, etc., and \texttt{kernel\_clone(...)} 
passes a special clone flags data structure. This function further calls \texttt{copy\_process(...)}, which is responsible for checking which 
flags are enabled using a bitwise \texttt{\&} operator and calling the corresponding copy utility; for example, \texttt{copy\_fs(...)}, \texttt{copy\_files(...)}, and \texttt{copy\_cred(...)}.
It also makes a call to \texttt{dup\_task\_struct(...)}, which will bind changes of the current \texttt{task\_struct} to a new one and return it back. After 
receiving
back a proper \texttt{task\_struct}, the \texttt{kernel\_clone(...)} schedules a fork for the newly created task.

However, DCE is not a complete architecture-level port and thus lacks a 
clear definition of \texttt{init\_task}. So, for the very first \texttt{kernel\_thread}, the current task would be \texttt{init\_task}, so not having a proper definition of it creates a domino effect making all subsequent tasks have an incomplete structure, leading to possible segmentation faults.  Furthermore, DCE does not use 
the kernel scheduler, but instead forks a kernel thread in such a way that the DCE scheduler can see it, and every time 
TaskManager::TaskWait(...)  is called on a particular task, DCE can put the current job to sleep, and give the other tasks/fibers a chance to 
execute their set of functions. 

Therefore, to take  control over the kernel thread creation, we rewrite the \texttt{kernel\_thread(...)} function to call \texttt{lib\_task\_start(...)}, which calls the 
internal TaskManager::Start(...) to create a DCE task, fills up the task\_struct, sets the SimTask context with the task\_struct and returns back 
the process ID (pid) of the task. Also, \texttt{current}, as previously referred to above, is not a variable, but a macro, which calls \texttt{get\_current(...)}, which DCE hijacks and leads to TaskManager::TaskCurrent(...), which checks if the current task has a SimTask context. If it does, it returns it back. 
If it does not, it calls TaskManager::Start(...) to set it up.
 

\subsubsection{Scheduler Workqueue/Waitqueue Implementation}
Most of the scheduling code can be found in the \texttt{/kernel} directory of the Linux kernel. DCe does not compile the 
original kernel implementation for the scheduler, but rather implements all of the key functions used by the networking stack.  One may ask,
if no applications are running inside the kernel, when would anything ever need to be scheduled?  The answer is that 
blocking network calls require such support.  Imagine creating a socket, binding and listening to it and then making an \texttt{accept(...)} call, and the client 
application doesn’t seem to ever come up. The system would go to a standstill if it did not schedule the client application for it to issue a 
corresponding \texttt{connect(...)} call. This is one of the issues with LKL [4.2], and is why we avoid linking those files, and rather pass the 
majority of the control to DCE, but without making a single change in code, because DCE also has to work when simulation scripts use the 
default ns-3 network stack.  DCE therefore rewrites certain functions. Below, a few function and concepts have been discussed : 

\begin{itemize}
\item schedule() : 
\item schedule\_timwout() :
\item lib\_task\_wait() :
\item lib\_task\_yield() : 
\item add\_timer() : 
\item mod\_timer() :
\item init\_timer\_key() :
\item do\_softirq() :
\item open\_softirq() :
\item queue\_work\_on() :
\end{itemize}

\subsubsection{Jiffies timer}
The Linux kernel management of time is based on the use of the global jiffies variable,
which contains a 32-bit integer that reports the number of ticks elapsed since the boot of
the kernel. The duration of each tick depends on the way the kernel was configured, but in modern Linux systems, it is usually configured to be one millisecond. The jiffies variable is normally
increased whenever the kernel timer interrupt is triggered. This timer interrupt is also
responsible for executing any expired kernel timers every ten milliseconds, etc.
[22, 19] both deal with this by executing periodically a per-node event that increases
the jiffies variable and relies on the Linux kernel code to deal with its internal timers. This
approach suffers from one major drawback: even if there are no timers scheduled to
expire for the next 10 or 80 milliseconds, the simulation will keep running and executing
events, just for the sake of incrementally increasing the value of this jiffies variable.
Rather than waste time to do this, DCE instead configures the Linux kernel to not use periodic
ticks with \texttt{CONFIG NOHZ} and then replaces the kernel timer facility entirely to schedule
simulation events for each kernel timer instead of keeping track of the kernel events in a
data structure separate from the main simulation event list. The resulting kernel network
stack thus runs in tickless mode and does not waste time scheduling unnecessary events.


\subsubsection{Kernel initialization}
The kernel initializers are divided into base init functions and roughly eight levels of initcalls, and export a start and end pointer of the list. 
Init functions are the ones directly called in the \texttt{start\_kernel(...)} or by certain device drivers, whereas initcalls are the ones which are stored 
in a special .initcall.init section of the final linked library. These are then copied to their respective positions through the linker.lds script.
In the \texttt{lib\_init} function, I call specific init functions, namely : 

\begin{itemize}
\item vfs\_caches\_init\_early
\item rcu\_init
\item devices\_init 
\item buses\_init
\item radix\_tree\_init
\item timekeeping\_init
\item cred\_init
\item uts\_ns\_init 
\item vfs\_caches\_init 
\item seq\_file\_init 
\item proc\_root\_init
\end{itemize}

I also changed the initcall invocation loop of lib\_init to this : 
\begin{lstlisting}[style=CStyle] 
   initcall_entry_t *call;
   extern initcall_entry_t __initcall_start[], __initcall_end[];
 
   call = __initcall_start;
   do {               
       initcall_from_entry(call)();               
       call++;
   } while (call < __initcall_end);

\end{lstlisting}

Before we call all the init functions, I also added these two lines of code :

\begin{lstlisting}[style=CStyle]
   files_cachep = kmem_cache_create("files_cache",
           sizeof(struct files_struct), 0,
           SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,
           NULL);
 
   fs_cachep = kmem_cache_create("fs_cache",
           sizeof(struct fs_struct), 0,
           SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,
           NULL);
\end{lstlisting}

These caches are required while setting up the \texttt{fs\_struct} and \texttt{files\_struct} for each new task in its corresponding \texttt{task\_struct}. Please see [5.9].


\subsubsection{Security LSM Module}
The security module of the Linux kernel depends on a special configuration called the \texttt{CONFIG\_LSM}. The LSM module requires a few architecture
defined variables. Surprisingly, one cannot find them in the source code (.c or .h files) anywhere; rather, they are placed in a specific section
of the library/binary called .init.data section, through assembly code. net-next-nuse makes use of a linker script which can help us position variables inside
the library under specific sections. I had to put this code in the linker.lds to initialise the start and end of the LSM table.
\begin{lstlisting}[style=CStyle]
  . = ALIGN(CONSTANT (MAXPAGESIZE));
  .init.data : AT(ADDR(.init.data)) {    
		__start_lsm_info = .;	
		KEEP(*(.lsm_info.init))
		__end_lsm_info = .;    
		__start_early_lsm_info = .;	
		KEEP(*(.early_lsm_info.init))
		__end_early_lsm_info = .;
  }
\end{lstlisting}
\texttt{vfs\_kern\_mount(...)} validates and parses params using \texttt{security\_fs\_context\_parse\_param(...)}, which checks if the requested operation was blocked
in the LSM tables. If this module is not enabled, it would return back ENOPARAM by default and fail.

\subsubsection{Kernel code and net-next-nuse alignment}
In updating net-next-nuse, the \texttt{panic\_on\_taint} symbol had to be removed.  This symbol is exported while linking kernel/panic.c (needed for acknowledging kernel
runtime errors) and accessing it in kernel/sysctl.c caused errors because it could not find it back. Also, including this symbol was unnecessary because its
value defaults to zero and never changes during program execution.

When the kernel is initialized, it calls \texttt{mnt\_init}, which is
responsible for setting
up the kernel file system (kernfs), the sysfs, ramfs, etc. and then making a
call to \texttt{init\_mount\_tree}, which sets up the required namespace for all
mounts. This function will then make a corresponding call to \texttt{vfs\_kern\_mount}, which will use the vfs setup to initialize the needed file systems.
\texttt{vfs\_kern\_mount} will set up the file context and mount it. In this process it checks for the security module and the LSM entries, to check
if the kernel was not previously configured to avoid declaration of the specified namespace. If all checks pass, it returns back a vfsmount
pointer. This pointer is used to set up a mount namespace which initializes the data structure \texttt{mnt\_namespace}, using \texttt{alloc\_mnt\_ns}. To
initialize the \texttt{mnt\_namespace} object, the current tasks nsproxy is required, which is the user namespace proxy member. Every task is required to
define some initial maximum value for each namespace it might require during the execution of the task. Similarly, for allocating a mount
namespace, it increases the \texttt{UCOUNT\_MNT\_NAMESPACES} of the current task’s nsproxy, which has an upper bound of \texttt{init\_task}‘s value.  Since
a complete architecture port is not needed, this value is not defined, and
would throw a segmentation fault, when \texttt{READ\_ONCE(...)} tries to make
an atomic operation on it to access it and increase the value by 1 if doing so doesn’t exceed the max limit, using the \texttt{atomic\_inc\_below(..)} macro.
We thus manually set the value for it, during task initialisation in \texttt{lib\_task\_start(...)}.

\begin{lstlisting}[style=CStyle]
ucounts->ns->ucount_max[UCOUNT_MNT_NAMESPACES] = MNS_MAX;
\end{lstlisting}

Although the mount namespace is set up for the init task, it is needed for other
tasks as well.  We therefore created a global variable \texttt{def\_mnt\_ns} and
stored a backup of the first mount namespace using an extern variable and then later on set it up in the \texttt{lib\_task\_start(...)}
\begin{lstlisting}[style=CStyle]
struct nsproxy *ns;
ns->uts_ns = 0;
ns->ipc_ns = 0;
ns->mnt_ns = def_mnt_ns;
ns->pid_ns_for_children = 0;
ns->net_ns = &init_net; // global struct * net   
task->kernel_task.nsproxy = ns;
\end{lstlisting}

This seems good right ? But what  is def\_root ? Ok, I had to hack this again XD.
Remember init\_mount\_tree, we discussed it in the pointer above. What does it do ?  It sets up a vfs mount. Okay, does that mean a path somewhere 
in the kernel ? Yes it should have. I also found another line of code there : 
init\_task.nsproxy->mnt\_ns = ns;

Okay, does this mean that the first task has the mount namespace generated while we mount and allocate the mount tree and acquire an mnt\_namespace. 
What else can we observe here ? How does the Linux Kernel create tasks then ? Do you remember doing these when you create processes on your OS ? 
No, right ? Which means, do we reuse some of the previous data from the init\_task and replicate them in new tasks. Yes, we do. Whenever we create 
a kernel\_thread(..) it calls copy\_process(...) which copies some of the required parameters from the host task, depending upon the flags you pass 
it. For example, If you had set the CLONE\_FS flag, it would copy the fs\_struct from the init\_task to the newly made task, by calling copy\_fs(..)
in kernel/fork.c. Which means, if we never have diverse task member requirements we can just reuse the previous task, implying the first task ?  
Right ! So, we can then just declare a global def\_root and then by declaring an extern variable for it, and keep reusing !

The netif\_napi\_add tests for the NAPI\_STATE\_LISTED bit to be enabled in the state of the napi\_struct passed on to it from cgroup init calls. 
We never set this flag directly, so this check was removed from the kernel code. sock\_init is one of the many initcalls which are called when 
sim\_init is invoked by DCE. First of all before we proceed with anything, it is necessary that we have initialised the proc file system completely,
so that the sysctl interface could be initialised by the net\_sysctl\_init(...). We then register the sockfs filesystem using register\_filesystem(...) 
and go for a kern\_mount(...) which invokes the VFS kern mount function and as discussed above, it requires the mnt\_init(...) to correctly setup 
the mount environment. Initially, in net-next-nuse-4.4.0, mnt\_init(...)  was made a blank function, and we have reasons to support it. 
In older kernel releases, sockfs file system init did not depend on a file system context, and required a mount function (sockfs\_mount) which
would directly mount the file system onto the kernel, but in commit [6.1], the socket file system mounting process was handed over to the
internal VFS mounting mechanism, thus breaking our setup. So, now we need to have the mnt\_init(...) and put in patches and glue codes wherever
needed.

TODO: include/arm changes to files, for example, atomic, barrier, ptrace, user, user32/64….

TODO: Rump kernel header file double include, adding flag to disable linux include

\subsubsection{Native Kernel NetDevices}
Certain components of ns-3 require the setup of custom NetDevices which require a custom MTU and other flags, such as whether the device should enable multicast, or is a point-to-point device, etc. These flags can be put together, and one can allocate a Linux netdevice struct for each such requirement using
the \texttt{alloc\_netdev(...)}, passing all of the configuration needed for details such as MTU, address length, and destructors, along with a \texttt{register\_netdev(...)} call.
This returns back a forward declared struct SimDevice which is used as a NetDevice object inside ns-3.

\begin{itemize}
 \item Requests are provided in the form of API functions ...
 \item Notifications are provided as callback functions ...
\end{itemize}

\section{Results}

\subsection{Docker vs Native DCE Simulation Tests}

\subsection{Performance : DCE vs. ns-3}

\subsection{Google BBR v1 Validation Results}

\section{Related Work}

\section{Conclusions}
%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{ltp-ns-3-workshop}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
